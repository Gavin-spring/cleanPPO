% !TEX root = ../Dissertation.tex

\appendix
\chapter{Software Manual}
\label{chap:software_manual}
\addcontentsline{toc}{chapter}{Software Manual}

This appendix provides a comprehensive guide to the software platform developed for this research. The platform is an integrated framework designed to handle the entire research pipeline, including procedural data generation, model training, and comparative evaluation of various solvers.

\section{Code Repository}
The source code is hosted on both GitHub for public access and the University of Birmingham's internal GitLab server for academic version control.

\begin{itemize}
    \item \small\textbf{Public GitHub Repository}: \url{https://github.com/Gavin-spring/ann_kp.git}
    \item \small\textbf{Internal Academic Repository}: \url{https://git.cs.bham.ac.uk/projects-2024-25/gxl386.git}
\end{itemize}

\section{Installation Guide}

\subsection{System Requirements}
\begin{itemize}
    \item \textbf{Operating System}: A Linux-based distribution is highly recommended. The framework leverages Triton to compile and optimize the neural network models, which currently has the most robust support on Linux.
    \item \textbf{Hardware}: An NVIDIA GPU is required for training and evaluation. The codebase is optimized for CUDA and supports mixed-precision training to accelerate performance.
    \item \textbf{NVIDIA CUDA}: CUDA version 12.1 or newer is required.
\end{itemize}

\subsection{Dependencies}
All required Python packages are listed in the \texttt{requirements.txt} file in the root of the repository. Key dependencies include PyTorch, Gymnasium, and Stable-Baselines3.

\subsection{Installation Steps}
A bash script, \texttt{setup\_env.sh}, is provided to automate the entire setup process. It is the recommended method for creating a clean and correct environment. Executing this script will perform the following actions:
\begin{enumerate}
    \item Download and install Miniconda, a minimal installer for the Conda package manager.
    \item Create a new Conda virtual environment to isolate project dependencies.
    \item Install all required packages listed in \texttt{requirements.txt} using `pip`.
    \item Install Rclone, a command-line tool for managing files on cloud storage, to facilitate data transfer.
    \item Install ngrok, a reverse proxy tool, to enable remote monitoring of training progress (e.g., via TensorBoard).
\end{enumerate}

\section{File Structure}
The repository is organized into a modular structure to separate concerns and improve maintainability.

\begin{description}
    \item[\texttt{data/}] Contains the datasets for training, validation, and testing. Each problem instance is stored in a `.csv` file.
    \item[\texttt{src/}] Contains the primary source code for the project, including:
        \begin{itemize}
            \item The implementation of the custom Gymnasium environment (\texttt{knapsack\_env.py}).
            \item The source code for all solvers, including classical algorithms and neural network architectures (\texttt{custom\_policy.py}, etc.).
        \end{itemize}
    \item[\texttt{scripts/}] Contains the main entry-point scripts for executing key pipeline stages, such as data generation, model training, and evaluation.
    \item[\texttt{artifacts/}] The default output directory for results generated by the \texttt{train\_model.py} and \texttt{evaluate\_solvers.py} scripts. This includes trained models, logs, and evaluation metrics for the MLP, Pointer Network (REINFORCE), and classical solvers.
    \item[\texttt{artifacts\_sb3/}] A dedicated output directory for all experiments related to the Stable-Baselines3 PPO implementation. Results from \texttt{train\_sb3.py} and \texttt{evaluate\_sb3.py} are stored here.
    \item[\texttt{docs/}] Contains supplementary documentation, including this manual.
    \item[\texttt{config.yaml}] A centralized configuration file for managing all hyperparameters, paths, and experimental settings.
    \item[\texttt{README.md}] The main project README file with an overview and basic instructions.
\end{description}

\section{Running the Code}
The project is packaged using \texttt{setup.py}, which creates convenient command-line entry points for all major scripts. After setting up the environment, the following commands become available system-wide:

\begin{verbatim}
    generate       # For generating new problem instances
    preprocess     # For any data preprocessing steps
    train          # For training the MLP and REINFORCE models
    train-sb3      # For training the final PPO model
    evaluate       # For evaluating all solvers in a unified test
    evaluate-sb3   # For dedicated evaluation of a PPO model
\end{verbatim}

\subsection{Data Generation}
To generate new datasets for the Knapsack Problem:
\begin{enumerate}
    \item Modify the \texttt{data\_gen} section of \texttt{config.yaml} to specify the desired characteristics of the problem instances (e.g., number of items, correlation type).
    \item Ensure the output paths in the `paths` section of \texttt{config.yaml} are set correctly.
    \item Run the command: \texttt{generate}
\end{enumerate}

\subsection{Model Training}
The framework supports training three different neural architectures. All hyperparameters and settings are managed through the \texttt{config.yaml} file.

\subsubsection{Training the Transformer-PPO Model (Primary Contribution)}
This is the primary model presented in this dissertation.
\begin{enumerate}
    \item Navigate to the \texttt{ml.rl.ppo} section in \texttt{config.yaml}.
    \item Adjust the model architecture and training hyperparameters as required. This includes settings for the Transformer encoder (\texttt{embedding\_dim}, \texttt{nhead}, \texttt{num\_layers}), the PPO algorithm (\texttt{n\_steps}, \texttt{gamma}, \texttt{clip\_range}), and the learning rate schedule.
    \item Verify that the \texttt{data\_training} and \texttt{data\_validation} paths are correct.
    \item Execute the training script using the command: \texttt{train-sb3 --name <YourExperimentName>}
\end{enumerate}

\subsubsection{Training the MLP Model}
\begin{enumerate}
    \item Navigate to the \texttt{ml.dnn} section in \texttt{config.yaml}.
    \item Configure the model and training hyperparameters, such as \texttt{total\_epochs}, \texttt{batch\_size}, and \texttt{learning\_rate}.
    \item Verify that the data paths are correct.
    \item Execute the training script using the command: \texttt{train} (ensure \texttt{training\_mode} is set to \texttt{dnn} in the config).
\end{enumerate}

\subsubsection{Training the Pointer Network (REINFORCE / Actor-Critic)}
\begin{enumerate}
    \item Navigate to the \texttt{ml.rl} section in \texttt{config.yaml}.
    \item Select the desired training algorithm by setting \texttt{training\_mode} to either \texttt{"reinforce"} (for EMA baseline) or \texttt{"actor\_critic"} (for critic baseline).
    \item Configure the hyperparameters under the corresponding section (\texttt{reinforce} or \texttt{actor\_critic}).
    \item Verify that the data paths are correct.
    \item Execute the training script using the command: \texttt{train}
\end{enumerate}

\subsection{Model Evaluation}
The framework provides two primary methods for evaluation.

\subsubsection{Dedicated PPO Model Evaluation}
To evaluate a trained PPO model against the Gurobi baseline on a test dataset:
\begin{enumerate}
    \item Execute the command, providing the path to the directory where the trained model and its statistics are saved: \\
    \texttt{evaluate-sb3 --run-dir /path/to/your/artifacts\_sb3/training/ExperimentRun/}
    \item Results, including performance plots and a `.csv` file with detailed metrics, will be saved in a new timestamped directory within \texttt{artifacts\_sb3/evaluation/}.
\end{enumerate}

\subsubsection{Unified Solver Evaluation}
To compare multiple solvers (classical, MLP, Ptr-Net, PPO) simultaneously:
\begin{enumerate}
    \item Open \texttt{src/utils/config\_loader.py} and ensure the solvers you wish to test are active in the \texttt{ALGORITHM\_REGISTRY}.
    \item Execute the \texttt{evaluate} command, providing paths to the trained neural network models: \\
    \texttt{evaluate --dnn-model-path <path> --rl-model-path <path> --ppo-run-dir <path>}
    \item Results are saved in the \texttt{artifacts/results} directory.
\end{enumerate}