% !TEX root = ../Dissertation.tex

\section{Evaluation and Discussion}
\label{sec:discussion}

The results presented in Section 5.2 confirm the effectiveness of our proposed Transformer-PPO framework.
This section provides a deeper analysis of these findings, interpreting the performance differences between the models, discussing the critical role of the underlying architecture and training algorithm, and addressing the limitations and implications of this work.

\subsection{Analysis of Solution Quality (MRE)}
The primary goal of this research was to develop a model capable of generalizing to large-scale problems from training on small ones.
Figure~\ref{fig:mre_vs_size} demonstrates that this objective was successfully met.

Our PPO-based model maintains a low and stable Mean Relative Error (MRE) below 30\% for instances up to $n=200$, showcasing high solution quality.
This success can be attributed to a synergy of three key factors:
\begin{itemize}
    \item \textbf{Algorithmic Superiority}: The PPO algorithm's Actor-Critic structure, which uses Temporal Difference (TD) updates to estimate advantages, provides a low-variance learning signal.
This is substantially more stable than the high-variance Monte Carlo returns used by the baseline Pointer Network's REINFORCE algorithm, which failed to learn an effective policy.
\item \textbf{Architectural Advantage}: The \textbf{Transformer} encoder is crucial for performance.
Its self-attention mechanism processes all items in parallel, capturing the global, combinatorial relationships within the problem instance.
This is a more powerful approach than the sequential processing of an LSTM, which was used in the baseline Pointer Network architecture.
\item \textbf{Framework Robustness}: The use of the \textbf{Stable-Baselines3} framework provides significant benefits, most notably the `VecNormalize` wrapper.
The automatic and adaptive normalization of both observations and rewards during training is a key stabilization technique that was absent in the baseline implementations.
\end{itemize}
In contrast, the baseline MLP model completely failed to generalize, confirming that a simple neural network cannot learn the complex, structured policy required for the Knapsack Problem. It is worth noting, however, that under different problem formulations, such as modeling virtual machine placement as a KP, simpler neural networks have been shown to outperform traditional metaheuristics, highlighting the importance of matching the model architecture to the specific problem representation \cite{abidSolving012023}.

\subsection{Analysis of Computational Efficiency (Inference Time)}
While solution quality is paramount, computational efficiency is critical for practical applications.

\subsubsection{Initial Findings and Architectural Overhead}
As shown in the initial time analysis in Figure~\ref{fig:time_vs_size}, our PPO model is orders of magnitude faster than the optimal Gurobi solver but slower than the neural baselines.
This overhead is expected and stems from two architectural realities:
\begin{itemize}
    \item The \textbf{Transformer} architecture is inherently more computationally intensive than the lightweight LSTM or MLP models.
    \item The Actor-Critic design requires forward passes through both the policy (actor) and value (critic) networks, adding computational cost compared to an actor-only method like REINFORCE.
\end{itemize}

\subsubsection{The Bottleneck: Iterative vs. Batched Evaluation}
A deeper investigation revealed that the primary bottleneck was not the model's architecture, but the evaluation method.
The standard Stable-Baselines3 evaluation loop processes instances iteratively (one by one), which fails to leverage the massive parallel processing power of a GPU.
To demonstrate the model's true inference potential, we developed a custom, proof-of-concept batch solver capable of processing hundreds of instances simultaneously.
The preliminary results of this solver are shown in Figure~\ref{fig:batch_solver_results}.
The time comparison (Figure~\ref{fig:batch_time}) shows that when properly batched, our PPO model's inference time becomes nearly constant and is significantly faster than all other models.
This confirms that the model architecture is highly efficient for parallel execution.
However, as shown in the MRE comparison (Figure~\ref{fig:batch_mre}), the current implementation of this batch solver contains a bug that severely degrades solution quality.
While this component is still under development, these preliminary results strongly indicate that the model's perceived slowness is an artifact of the evaluation script, not a fundamental limitation of the model itself.

\begin{figure}[H]
    \centering
    % Set the width of the subfigure to be larger, which forces a vertical stack.
    % A width around 0.7 to 0.8\textwidth is usually good for vertical layouts.
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/batch_errors.png}
        \caption{MRE vs. Problem Size (Batched)}
        \label{fig:batch_mre}
    \end{subfigure}
    
    % A blank line or \vspace adds vertical separation between the figures.
    \vspace{1em} 
    
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/batch_time.png}
        \caption{Inference Time vs. Problem Size (Batched)}
        \label{fig:batch_time}
    \end{subfigure}
    
    % This is the main caption for the entire figure.
    \caption{Preliminary results from a custom-built batch evaluation solver. (a) The current implementation shows a degradation in solution quality, which is a work-in-progress.
(b) However, it successfully demonstrates that the PPO model's inference time becomes nearly constant and extremely fast when leveraging batch processing.}
    \label{fig:batch_solver_results}
\end{figure}

\subsection{Analysis of a High-Performance Batched Solver}
\label{sec:analysis_batch_solver}

A primary motivation for employing neural networks in combinatorial optimization is their potential for massive parallelization.
To investigate this, we developed a prototype high-performance solver \texttt{fast\_ppo\_solver} designed to process entire batches of problems simultaneously, aiming for a near-constant inference time, as detailed in Algorithm~\ref{alg:batch_solver}.
The empirical results, presented in Figure~\ref{fig:batch_solver_results}, confirm this solver's speed potential.
As shown in Figure~\ref{fig:batch_time}, the inference time remains constant regardless of problem size, demonstrating the efficiency of leveraging GPU hardware for parallel computation.
However, this gain in speed was accompanied by a severe degradation in solution quality, with the MRE rising dramatically (Figure~\ref{fig:batch_mre}).
This discrepancy between the two solvers provides a critical insight into a common challenge in machine learning deployment: the \textbf{Training-Inference Skew}.

The core of the issue lies in the fundamental difference between the data processing workflows of the two solvers.
The successful instance-based solver (Algorithm~\ref{alg:instance_solver}) operates in a stateful, step-by-step loop.
Crucially, after each item selection, the environment's state (including the \textit{remaining capacity}) is updated, and this entire new state is re-normalized and re-processed by the model's feature extractor.
The model's policy is therefore conditioned on a dynamically updated state representation at every step.
In contrast, the batched solver (Algorithm~\ref{alg:batch_solver}), in its pursuit of efficiency, performs feature extraction and normalization only once on the initial problem state.
Its internal autoregressive decoding loop, while tracking capacity for constraint validation, does not feed this updated state information back into the feature extractor.
Consequently, every decision after the first is based on a stale feature representation derived from the initial state.
The model, which was trained to expect a fully updated and re-normalized state at each step, is thus forced to operate on a data distribution it has never seen, leading to a catastrophic failure of its learned policy.
This finding underscores the critical importance of maintaining strict consistency between the data processing pipelines used during training and inference to ensure a model's performance translates from the research environment to practical application.

\begin{algorithm}[htbp]
    \caption{Inference Procedure for the Batched Solver}
    \label{alg:batch_solver}
    \KwIn{Trained PPO Model $\pi_\theta$, Batch of problem instances $\mathcal{B}$}
    \KwOut{Batch of final solution values $\mathcal{V}$}
    \BlankLine
    \tcp{--- Phase 1: Pre-processing and One-Time Feature Extraction ---}
    $s_0 \leftarrow \text{Construct observation dictionary for batch } \mathcal{B}$\;
    $s_0^{\text{norm}} \leftarrow \text{Manually scale and normalize features in } s_0$\;
    $C \leftarrow \mathrm{FeatureExtractor}(s_0^{\text{norm}})$\;
    \BlankLine
    \tcp{--- Phase 2: Internal Autoregressive Decoding ---}
    $\mathcal{A} \leftarrow \text{empty list}$\;
    Initialize internal capacity tracker and masks\;
    \BlankLine
    \While{decoding is not complete for all instances in batch}{
        \tcp{Decision is based on the initial context C}
        $a_t \leftarrow \mathrm{select\_action}(C, \mathrm{internal\_query\_vector})$\;
        Append $a_t$ to $\mathcal{A}$\;
        \BlankLine
        \tcp{Internal state is updated for constraints only}
        Update internal capacity tracker and masks based on $a_t$\;
        \tcp{Note: The context C is not re-computed}
    }
    \BlankLine
    $\mathcal{V} \leftarrow \text{Post-process sequence } \mathcal{A} \text{ to calculate final values}$\;
    \Return{$\mathcal{V}$}\;
\end{algorithm}

\subsection{Synthesis and Answering Research Questions}
The experimental results, when synthesized, provide comprehensive answers to our core research questions and highlight the trade-offs inherent in deploying complex learned models.

\subsubsection{On Generalization and Solution Quality}
The primary research question concerned the model's ability to generalize from small-scale training instances to larger, unseen problems.
The performance of the instance-based solver (Section~\ref{sec:results}) provides a clear, affirmative answer.
By maintaining a stable, low MRE on instances up to four times the size of its training data, the model demonstrates that it has learned a genuinely scalable and robust policy, successfully avoiding the pitfalls of overfitting to fixed problem dimensions that have limited prior neural approaches.

\subsubsection{On Computational Efficiency and Scalability}
Our second research question addressed the framework's potential for high-speed inference.
The two solver implementations offer complementary insights. The instance-based solver shows favorable scaling relative to exact methods, confirming its practicality.
More significantly, the batched solver experiment (Section~\ref{sec:analysis_batch_solver}) definitively proves the architectural potential for constant-time, $O(1)$ inference.
This result confirms that the Transformer-based model is not just a theoretical construct but is well-suited for high-throughput, real-world applications where decision speed is critical.

\subsubsection{Synthesis: A Robust Framework with Critical Deployment Insights}
In conclusion, the combination of a Transformer architecture with the PPO algorithm is a potent and effective choice for this class of combinatorial optimization problem.
The framework successfully learns a generalizable policy that delivers high-quality solutions.
However, the stark performance contrast between the instance-based and batched solvers reveals a critical finding: the learned policy's effectiveness is deeply coupled to the state representation and processing pipeline used during training.
This discovery of a significant training-inference skew provides a crucial insight for the field, emphasizing that the design of an inference engine is not merely an implementation detail but a core component that must be co-designed with the model and training regime to ensure success.