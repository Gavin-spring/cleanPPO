@article{abidSolving012023,
  title = {Solving the 0/1 {{Knapsack Problem Using Metaheuristic}} and {{Neural Networks}} for the {{Virtual Machine Placement Process}} in {{Cloud Computing Environment}}},
  author = {Abid, Mohamed and El Kafhali, Said and Amzil, Abdellah and Hanini, Mohamed},
  editor = {Gao, Hao},
  year = {2023},
  month = jan,
  journal = {Mathematical Problems in Engineering},
  volume = {2023},
  number = {1},
  pages = {1742922},
  issn = {1024-123X, 1563-5147},
  doi = {10.1155/2023/1742922},
  urldate = {2025-05-03},
  abstract = {Virtual machine placement (VMP) is carried out during virtual machine migration to choose the best physical computer to host the virtual machines. It is a crucial task in cloud computing. It directly affects data center performance, resource utilization, and power consumption, and it can help cloud providers save money on data center maintenance. To optimize various characteristics that affect data centers, VMs, and their runs, numerous VMP strategies have been developed in the cloud computing environment. This paper aims to compare the accuracy and efficiency of nine distinct strategies for treating the VMP as a knapsack problem. In the numerical analysis, we test out various conditions to determine how well the system works. We first illustrate the rate of convergence for algorithms, then the rate of execution time growth for a given number of virtual machines, and lastly the rate of development of CPU usage rate supplied by the nine methods throughout the three analyzed conditions. The obtained results reveal that the neural network algorithm performs better than the other eight approaches. The model performed well, as shown by its ability to provide near-optimal solutions to test cases.},
  langid = {english},
  keywords = {/unread},
  annotation = {TLDR: This paper aims to compare the accuracy and efficiency of nine distinct strategies for treating the VMP as a knapsack problem, and reveals that the neural network algorithm performs better than the other eight approaches.},
  file = {C:\Users\LinG\Zotero\storage\V6HJ5725\Abid et al. - 2023 - Solving the 01 Knapsack Problem Using Metaheuristic and Neural Networks for the Virtual Machine Pla.pdf}
}

@misc{afsharStateAggregationApproach2020,
  title = {A {{State Aggregation Approach}} for {{Solving Knapsack Problem}} with {{Deep Reinforcement Learning}}},
  author = {Afshar, Reza Refaei and Zhang, Yingqian and Firat, Murat and Kaymak, Uzay},
  year = {2020},
  month = apr,
  number = {arXiv:2004.12117},
  eprint = {2004.12117},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.12117},
  urldate = {2025-06-28},
  abstract = {This paper proposes a Deep Reinforcement Learning (DRL) approach for solving knapsack problem. The proposed method consists of a state aggregation step based on tabular reinforcement learning to extract features and construct states. The state aggregation policy is applied to each problem instance of the knapsack problem, which is used with Advantage Actor Critic (A2C) algorithm to train a policy through which the items are sequentially selected at each time step. The method is a constructive solution approach and the process of selecting items is repeated until the final solution is obtained. The experiments show that our approach provides close to optimal solutions for all tested instances, outperforms the greedy algorithm, and is able to handle larger instances and more flexible than an existing DRL approach. In addition, the results demonstrate that the proposed model with the state aggregation strategy not only gives better solutions but also learns in less timesteps, than the one without state aggregation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,/reading,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {remark: e2e-code-aggregation for state explosion + Advantage Actor-Critic + Q-l(tabulation)},
  file = {C:\Users\LinG\Zotero\storage\E4VEW78Y\Afshar et al. - 2020 - A State Aggregation Approach for Solving Knapsack Problem with Deep Reinforcement Learning.pdf}
}

@misc{belloNeuralCombinatorialOptimization2017,
  title = {Neural {{Combinatorial Optimization}} with {{Reinforcement Learning}}},
  author = {Bello, Irwan and Pham, Hieu and Le, Quoc V. and Norouzi, Mohammad and Bengio, Samy},
  year = {2017},
  month = jan,
  number = {arXiv:1611.09940},
  eprint = {1611.09940},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1611.09940},
  urldate = {2025-06-12},
  abstract = {This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent neural network that, given a set of city coordinates, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent neural network using a policy gradient method. We compare learning the network parameters on a set of training graphs against learning them on individual test graphs. Despite the computational expense, without much engineering and heuristic designing, Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied to the KnapSack, another NP-hard problem, the same method obtains optimal solutions for instances with up to 200 items.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,/reading,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {remark: e2e-code-RL(policy gradient) + Pointer Network(RNN)},
  file = {C:\Users\LinG\Zotero\storage\AEWW7ZAV\Bello et al. - 2017 - Neural Combinatorial Optimization with Reinforcement Learning.pdf}
}

@article{buayenEfficient01MultipleKnapsack2022,
  title = {Efficient 0/1-{{Multiple-Knapsack Problem Solving}} by {{Hybrid DP Transformation}} and {{Robust Unbiased Filtering}}},
  author = {Buayen, Patcharin and Werapun, Jeeraporn},
  year = {2022},
  month = sep,
  journal = {Algorithms},
  volume = {15},
  number = {10},
  pages = {366},
  issn = {1999-4893},
  doi = {10.3390/a15100366},
  urldate = {2025-04-29},
  abstract = {The multiple knapsack problem (0/1-mKP) is a valuable NP-hard problem involved in many science-and-engineering applications. In current research, there exist two main approaches: 1. the exact algorithms for the optimal solutions (i.e., branch-and-bound, dynamic programming (DP), etc.) and 2. the approximate algorithms in polynomial time (i.e., Genetic algorithm, swarm optimization, etc.). In the past, the exact-DP could find the optimal solutions of the 0/1-KP (one knapsack, n objects) in O(nC). For large n and massive C, the unbiased filtering was incorporated with the exact-DP to solve the 0/1-KP in O(n + C{$\prime$}) with 95\% optimal solutions. For the complex 0/1-mKP (m knapsacks) in this study, we propose a novel research track with hybrid integration of DP-transformation (DPT), exact-fit (best) knapsack order (m!-to-m2 reduction), and robust unbiased filtering. First, the efficient DPT algorithm is proposed to find the optimal solutions for each knapsack in O([n2,nC]). Next, all knapsacks are fulfilled by the exact-fit (best) knapsack order in O(m2[n2,nC]) over O(m![n2,nC]) while retaining at least 99\% optimal solutions as m! orders. Finally, robust unbiased filtering is incorporated to solve the 0/1-mKP in O(m2n). In experiments, our efficient 0/1-mKP reduction confirmed 99\% optimal solutions on random and benchmark datasets (n {$\delta$} 10,000, m {$\delta$} 100).},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {,/unread},
  annotation = {TLDR: A novel research track with hybrid integration of DP-transformation (DPT), exact-fit (best) knapsack order (m!-to-m2 reduction), and robust unbiased filtering is proposed to solve the 0/1-mKP in O(m2n).},
  file = {C:\Users\LinG\Zotero\storage\926EK4SI\Buayen and Werapun - 2022 - Efficient 01-Multiple-Knapsack Problem Solving by Hybrid DP Transformation and Robust Unbiased Filt.pdf}
}

@misc{cappartCombinatorialOptimizationReasoning2022,
  title = {Combinatorial Optimization and Reasoning with Graph Neural Networks},
  author = {Cappart, Quentin and Ch{\'e}telat, Didier and Khalil, Elias and Lodi, Andrea and Morris, Christopher and Veli{\v c}kovi{\'c}, Petar},
  year = {2022},
  month = sep,
  number = {arXiv:2102.09544},
  eprint = {2102.09544},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2102.09544},
  urldate = {2025-04-29},
  abstract = {Combinatorial optimization is a well-established area in operations research and computer science. Until recently, its methods have focused on solving problem instances in isolation, ignoring that they often stem from related data distributions in practice. However, recent years have seen a surge of interest in using machine learning, especially graph neural networks (GNNs), as a key building block for combinatorial tasks, either directly as solvers or by enhancing exact solvers. The inductive bias of GNNs effectively encodes combinatorial and relational input due to their invariance to permutations and awareness of input sparsity. This paper presents a conceptual review of recent key advancements in this emerging field, aiming at optimization and machine learning researchers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,/unread,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\LinG\\Zotero\\storage\\HI4HYYVX\\Cappart et al. - 2022 - Combinatorial optimization and reasoning with graph neural networks.pdf;C\:\\Users\\LinG\\Zotero\\storage\\I3JGLQ8P\\Book-all-in-one.pdf}
}

@article{cappartCombiningReinforcementLearning2021,
  title = {Combining {{Reinforcement Learning}} and {{Constraint Programming}} for {{Combinatorial Optimization}}},
  author = {Cappart, Quentin and Moisan, Thierry and Rousseau, Louis-Martin and {Pr{\'e}mont-Schwarz}, Isabeau and Cire, Andre A.},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {5},
  pages = {3677--3687},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v35i5.16484},
  urldate = {2025-06-28},
  abstract = {Combinatorial optimization has found applications in numerous fields, from aerospace to transportation planning and economics. The goal is to find an optimal solution among a finite set of possibilities. The well-known challenge one faces with combinatorial optimization is the state-space explosion problem: the number of possibilities grows exponentially with the problem size, which makes solving intractable for large problems. In the last years, deep reinforcement learning (DRL) has shown its promise for designing good heuristics dedicated to solve NP-hard combinatorial optimization problems. However, current approaches have an important shortcoming: they only provide an approximate solution with no systematic ways to improve it or to prove optimality. In another context, constraint programming (CP) is a generic tool to solve combinatorial optimization problems. Based on a complete search procedure, it will always find the optimal solution if we allow an execution time large enough. A critical design choice, that makes CP non-trivial to use in practice, is the branching decision, directing how the search space is explored. In this work, we propose a general and hybrid approach, based on DRL and CP, for solving combinatorial optimization problems. The core of our approach is based on a dynamic programming formulation, that acts as a bridge between both techniques. We experimentally show that our solver is efficient to solve three challenging problems: the traveling salesman problem with time windows, the 4-moments portfolio optimization problem, and the 0-1 knapsack problem. Results obtained show that the framework introduced outperforms the stand-alone RL and CP solutions, while being competitive with industrial solvers.},
  langid = {english},
  keywords = {,/reading},
  annotation = {TLDR: This work proposes a general and hybrid approach, based on DRL and CP, that acts as a bridge between both techniques, and experimentally shows that the framework introduced outperforms the stand-alone RL and CP solutions, while being competitive with industrial solvers.\\
remark: guide-code-PPO/DQN + Gecode},
  file = {C:\Users\LinG\Zotero\storage\DYTAVKFT\Cappart et al. - 2021 - Combining Reinforcement Learning and Constraint Programming for Combinatorial Optimization.pdf}
}

@book{gervasiComputationalScienceIts2016,
  title = {Computational {{Science}} and {{Its Applications}} -- {{ICCSA}} 2016: 16th {{International Conference}}, {{Beijing}}, {{China}}, {{July}} 4-7, 2016, {{Proceedings}}, {{Part II}}},
  shorttitle = {Computational {{Science}} and {{Its Applications}} -- {{ICCSA}} 2016},
  editor = {Gervasi, Osvaldo and Murgante, Beniamino and Misra, Sanjay and Rocha, Ana Maria A.C. and Torre, Carmelo M. and Taniar, David and Apduhan, Bernady O. and Stankova, Elena and Wang, Shangguang},
  year = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {9787},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-42108-7},
  urldate = {2025-04-29},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-319-42107-0 978-3-319-42108-7},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\LinG\Zotero\storage\WUXGRYRJ\Gervasi et al. - 2016 - Computational Science and Its Applications – ICCSA 2016 16th International Conference, Beijing, Chi.pdf}
}

@misc{gholipourTPTOTransformerPPOBased2023,
  title = {{{TPTO}}: {{A Transformer-PPO}} Based {{Task Offloading Solution}} for {{Edge Computing Environments}}},
  shorttitle = {{{TPTO}}},
  author = {Gholipour, Niloofar and de Assuncao, Marcos Dias and Agarwal, Pranav and {gascon-samson}, julien and Buyya, Rajkumar},
  year = {2023},
  month = dec,
  number = {arXiv:2312.11739},
  eprint = {2312.11739},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.11739},
  urldate = {2025-07-23},
  abstract = {Emerging applications in healthcare, autonomous vehicles, and wearable assistance require interactive and low-latency data analysis services. Unfortunately, cloud-centric architectures cannot fulfill the low-latency demands of these applications, as user devices are often distant from cloud data centers. Edge computing aims to reduce the latency by enabling processing tasks to be offloaded to resources located at the network's edge. However, determining which tasks must be offloaded to edge servers to reduce the latency of application requests is not trivial, especially if the tasks present dependencies. This paper proposes a DRL approach called TPTO, which leverages Transformer Networks and PPO to offload dependent tasks of IoT applications in edge computing. We consider users with various preferences, where devices can offload computation to an edge server via wireless channels. Performance evaluation results demonstrate that under fat application graphs, TPTO is more effective than state-of-the-art methods, such as Greedy, HEFT, and MRLCO, by reducing latency by 30.24\%, 29.61\%, and 12.41\%, respectively. In addition, TPTO presents a training time approximately 2.5 times faster than an existing DRL approach.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,Computer Science - Distributed Parallel and Cluster Computing},
  file = {C:\Users\LinG\Zotero\storage\GZ3ETVYS\Gholipour et al. - 2023 - TPTO A Transformer-PPO based Task Offloading Solution for Edge Computing Environments.pdf}
}

@article{hertrichProvablyGoodSolutions2023,
  title = {Provably {{Good Solutions}} to the {{Knapsack Problem}} via {{Neural Networks}} of {{Bounded Size}}},
  author = {Hertrich, Christoph and Skutella, Martin},
  year = {2023},
  month = sep,
  journal = {INFORMS Journal on Computing},
  volume = {35},
  number = {5},
  eprint = {2005.14105},
  primaryclass = {cs},
  pages = {1079--1097},
  issn = {1091-9856, 1526-5528},
  doi = {10.1287/ijoc.2021.0225},
  urldate = {2025-04-29},
  abstract = {The development of a satisfying and rigorous mathematical understanding of the performance of neural networks is a major challenge in artificial intelligence. Against this background, we study the expressive power of neural networks through the example of the classical NP-hard Knapsack Problem. Our main contribution is a class of recurrent neural networks (RNNs) with rectified linear units that are iteratively applied to each item of a Knapsack instance and thereby compute optimal or provably good solution values. We show that an RNN of depth four and width depending quadratically on the profit of an optimum Knapsack solution is sufficient to find optimum Knapsack solutions. We also prove the following tradeoff between the size of an RNN and the quality of the computed Knapsack solution: for Knapsack instances consisting of \$n\$ items, an RNN of depth five and width \$w\$ computes a solution of value at least \$1-{\textbackslash}mathcal\{O\}(n{\textasciicircum}2/{\textbackslash}sqrt\{w\})\$ times the optimum solution value. Our results build upon a classical dynamic programming formulation of the Knapsack Problem as well as a careful rounding of profit values that are also at the core of the well-known fully polynomial-time approximation scheme for the Knapsack Problem. A carefully conducted computational study qualitatively supports our theoretical size bounds. Finally, we point out that our results can be generalized to many other combinatorial optimization problems that admit dynamic programming solution methods, such as various Shortest Path Problems, the Longest Common Subsequence Problem, and the Traveling Salesperson Problem.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,/done,Computer Science - Computational Complexity,Computer Science - Discrete Mathematics,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annotation = {TLDR: A class of recurrent neural networks with rectified linear units that are iteratively applied to each item of a Knapsack instance and thereby compute optimal or provably good solution values, which are generalized to many other combinatorial optimization problem methods.\\
remark: notRL: RNN},
  file = {C\:\\Users\\LinG\\Zotero\\storage\\ADEIVZSL\\1802.04240v2.pdf;C\:\\Users\\LinG\\Zotero\\storage\\QA845LDD\\Hertrich and Skutella - 2023 - Provably Good Solutions to the Knapsack Problem via Neural Networks of Bounded Size.pdf}
}

@article{hopfieldNeuralComputationDecisions1985,
  title = {``{{Neural}}'' Computation of Decisions in Optimization Problems},
  author = {Hopfield, J. J. and Tank, D. W.},
  year = {1985},
  month = jul,
  journal = {Biological Cybernetics},
  volume = {52},
  number = {3},
  pages = {141--152},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00339943},
  urldate = {2025-06-12},
  copyright = {https://www.springer.com/tdm},
  langid = {english},
  keywords = {/unread},
  annotation = {TLDR: Results of computer simulations of a network designed to solve a difficult but well-defined optimization problem-the Traveling-Salesman Problem-are presented and used to illustrate the computational power of the networks.},
  file = {C:\Users\LinG\Zotero\storage\YNRMDU8C\Hopfield and Tank - 1985 - “Neural” computation of decisions in optimization problems.pdf}
}

@article{khalilLearningCombinatorialOptimization2017,
  title = {Learning {{Combinatorial Optimization Algorithms}} over {{Graphs}}},
  author = {Khalil, Elias and Dai, Hanjun and Zhang, Yuyu and Dilkina, Bistra and Song, Le},
  year = {2017},
  month = sep,
  abstract = {The design of good heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires significant specialized knowledge and trial-and-error. Can we automate this challenging, tedious process, and learn the algorithms instead? In many real-world applications, it is typically the case that the same optimization problem is solved again and again on a regular basis, maintaining the same problem structure but differing in the data. This provides an opportunity for learning heuristic algorithms that exploit the structure of such recurring problems. In this paper, we propose a unique combination of reinforcement learning and graph embedding to address this challenge. The learned greedy policy behaves like a meta-algorithm that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show that our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\LinG\Zotero\storage\4LW8HCZ8\Khalil et al. - Learning Combinatorial Optimization Algorithms over Graphs.pdf}
}

@article{koolATTENTIONLEARNSOLVE2019,
  title = {{{ATTENTION}}, {{LEARN TO SOLVE ROUTING PROBLEMS}}!},
  author = {Kool, Wouter and {van Hoof}, Herke and Welling, Max},
  year = {2019},
  abstract = {The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development. However, to push this idea towards practical implementation, we need better models and better ways of training. We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function. We significantly improve over recent learned heuristics for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes. With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms.},
  langid = {english},
  keywords = {,/reading},
  annotation = {remark: e2e-code-Multi-head Attention + REINFORCE with a Greedy Rollout Baseline},
  file = {C:\Users\LinG\Zotero\storage\WLPBIUKZ\Kool et al. - 2019 - ATTENTION, LEARN TO SOLVE ROUTING PROBLEMS!.pdf}
}

@misc{koolAttentionLearnSolve2019a,
  title = {Attention, {{Learn}} to {{Solve Routing Problems}}!},
  author = {Kool, Wouter and van Hoof, Herke and Welling, Max},
  year = {2019},
  month = feb,
  number = {arXiv:1803.08475},
  eprint = {1803.08475},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.08475},
  urldate = {2025-06-28},
  abstract = {The recently presented idea to learn heuristics for combinatorial optimization problems is promising as it can save costly development. However, to push this idea towards practical implementation, we need better models and better ways of training. We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train this model using REINFORCE with a simple baseline based on a deterministic greedy rollout, which we find is more efficient than using a value function. We significantly improve over recent learned heuristics for the Travelling Salesman Problem (TSP), getting close to optimal results for problems up to 100 nodes. With the same hyperparameters, we learn strong heuristics for two variants of the Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of baselines and getting results close to highly optimized and specialized algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {/unread,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\LinG\Zotero\storage\N26VWDZJ\Kool et al. - 2019 - Attention, Learn to Solve Routing Problems!.pdf}
}

@article{leeDeepNeuralNetworks2018,
  title = {Deep {{Neural Networks}} for {{Linear Sum Assignment Problems}}},
  author = {Lee, Mengyuan and Xiong, Yuanhao and Yu, Guanding and Li, Geoffrey Ye},
  year = {2018},
  month = dec,
  journal = {IEEE Wireless Communications Letters},
  volume = {7},
  number = {6},
  pages = {962--965},
  issn = {2162-2337, 2162-2345},
  doi = {10.1109/LWC.2018.2843359},
  urldate = {2025-06-12},
  abstract = {Many resource allocation issues in wireless communications can be modeled as assignment problems and can be solved online with global information. However, traditional methods for assignment problems take a lot of time to find the optimal solutions. In this letter, we solve the assignment problem using machine learning (ML) approach. Specifically, the linear sum assignment problems (LSAPs) are solved by the deep neural networks (DNNs). Since LSAP is a combinatorial optimization problem, it is first decomposed into several subassignment problems. Each of them is a classification problem and can be solved effectively with DNNs. Two kinds of DNNs, feed-forward neural network (FNN) and convolutional neural network (CNN), are implemented to deal with the sub-assignment problems, respectively. Based on computer simulation, DNNs can effectively solve LSAPs with great time efficiency and only slight loss of accuracy.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/OAPA.html},
  langid = {english},
  keywords = {/unread},
  annotation = {TLDR: This letter solves the assignment problem using machine learning approach, and the linear sum assignment problems (LSAPs) are solved by the deep neural networks (DNNs).},
  file = {C:\Users\LinG\Zotero\storage\UHMF3WLA\Lee et al. - 2018 - Deep Neural Networks for Linear Sum Assignment Problems.pdf}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14236},
  urldate = {2025-06-28},
  langid = {english},
  keywords = {,/unread},
  annotation = {TLDR: This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
  file = {C:\Users\LinG\Zotero\storage\JDIIV5ID\Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf}
}

@misc{nazariReinforcementLearningSolving2018,
  title = {Reinforcement {{Learning}} for {{Solving}} the {{Vehicle Routing Problem}}},
  author = {Nazari, Mohammadreza and Oroojlooy, Afshin and Snyder, Lawrence V. and Tak{\'a}{\v c}, Martin},
  year = {2018},
  month = may,
  number = {arXiv:1802.04240},
  eprint = {1802.04240},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.04240},
  urldate = {2025-06-12},
  abstract = {We present an end-to-end framework for solving the Vehicle Routing Problem (VRP) using reinforcement learning. In this approach, we train a single model that finds near-optimal solutions for problem instances sampled from a given distribution, only by observing the reward signals and following feasibility rules. Our model represents a parameterized stochastic policy, and by applying a policy gradient algorithm to optimize its parameters, the trained model produces the solution as a sequence of consecutive actions in real time, without the need to re-train for every new problem instance. On capacitated VRP, our approach outperforms classical heuristics and Google's OR-Tools on medium-sized instances in solution quality with comparable computation time (after training). We demonstrate how our approach can handle problems with split delivery and explore the effect of such deliveries on the solution quality. Our proposed framework can be applied to other variants of the VRP such as the stochastic VRP, and has the potential to be applied more generally to combinatorial optimization problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,unread},
  annotation = {titleTranslation: 用于解决车辆路线问题的强化学习},
  file = {C\:\\Users\\LinG\\Zotero\\storage\\J39WN7YB\\1707.06347v2.pdf;C\:\\Users\\LinG\\Zotero\\storage\\L34B6KSP\\1-s2.0-S0957417422021716-main.pdf;C\:\\Users\\LinG\\Zotero\\storage\\MI4L2PLB\\2312.11739v1.pdf;C\:\\Users\\LinG\\Zotero\\storage\\NQXFXH8K\\Nazari et al. - 2018 - Reinforcement Learning for Solving the Vehicle Routing Problem.pdf;C\:\\Users\\LinG\\Zotero\\storage\\WMWBUTNE\\Solving_Combinatorial_Optimization_Problems_with_Deep_Neural_Network_A_Survey.pdf;C\:\\Users\\LinG\\Zotero\\storage\\XLZQJSWE\\1-s2.0-S0360835224008350-main.pdf}
}

@article{ohlssonNeuralNetworksOptimization1993,
  title = {Neural {{Networks}} for {{Optimization Problems}} with {{Inequality Constraints}}: {{The Knapsack Problem}}},
  shorttitle = {Neural {{Networks}} for {{Optimization Problems}} with {{Inequality Constraints}}},
  author = {Ohlsson, Mattias and Peterson, Carsten and S{\"o}derberg, Bo},
  year = {1993},
  month = mar,
  journal = {Neural Computation},
  volume = {5},
  number = {2},
  pages = {331--339},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1993.5.2.331},
  urldate = {2025-06-12},
  abstract = {Astrategyforndingapproximatesolutionstodiscreteoptimizationproblemswithinequalityconstraintsusingmeaneldneuralnetworksispresented.Theconstraintsx0areencodedbyx(x)terms intheenergyfunction.Acarefultreatmentofthemeaneldapproximationfortheself-couplingpartsof theenergyiscrucial,andresultsinanessentiallyparameter-freealgorithm.},
  langid = {english},
  keywords = {/unread},
  annotation = {TLDR: A strategy for finding approximate solutions to discrete optimization problems with inequality constraints using mean field neural networks is presented, and results in an essentially parameter-free algorithm on the knapsack problem of size up to 103 items.},
  file = {C:\Users\LinG\Zotero\storage\NCHP5APP\Ohlsson et al. - 1993 - Neural Networks for Optimization Problems with Inequality Constraints The Knapsack Problem.pdf}
}

@article{queSolving3DPacking2023,
  title = {Solving {{3D}} Packing Problem Using {{Transformer}} Network and Reinforcement Learning},
  author = {Que, Quanqing and Yang, Fang and Zhang, Defu},
  year = {2023},
  month = mar,
  journal = {Expert Systems with Applications},
  volume = {214},
  pages = {119153},
  publisher = {Elsevier BV},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2022.119153},
  urldate = {2025-07-23},
  abstract = {The three-dimensional packing problem (3D-PP) is a classic NP-hard problem in operations research and computer science. One of the most popular ways to solve the problem is heuristic methods with a search strategy. However, approaches based on machine learning have recently received widespread attention because of their efficiency. In this work, we propose a deep reinforcement learning (DRL) model to solve 3D-PP. Our method employs Transformer architecture as the policy network and uses Proximal Policy Optimization (PPO) to train the network. Compared with previous approaches using DRL, our method presents a novel state representation of packing environment, and introduces plane features for representing the length and width information of container. Our method achieves the new state-of-the-art results for using DRL to solve 3D-PP. The code of our method will be released to facilitate future research.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {C:\Users\LinG\Zotero\storage\NR8IG4BD\Que et al. - 2023 - Solving 3D packing problem using Transformer network and reinforcement learning.pdf}
}

@article{reyes-aldasoroHybridModelBased1999,
  title = {A Hybrid Model Based on Dynamic Programming, Neural Networks, and Surrogate Value for Inventory Optimisation Applications},
  author = {{Reyes-Aldasoro}, C C and Ganguly, A R and Lemus, G and Gupta, A},
  year = {1999},
  month = jan,
  journal = {Journal of the Operational Research Society},
  volume = {50},
  number = {1},
  pages = {85--94},
  issn = {0160-5682, 1476-9360},
  doi = {10.1057/palgrave.jors.2600658},
  urldate = {2025-04-29},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  keywords = {/unread},
  annotation = {TLDR: This framework combines two existing approaches and introduces a new associated cost parameter that serves as a surrogate for customer satisfaction that can be utilised for analysis, modelling and forecasting purposes.},
  file = {C:\Users\LinG\Zotero\storage\DNP9BMZL\Reyes-Aldasoro et al. - 1999 - A hybrid model based on dynamic programming, neural networks, and surrogate value for inventory opti.pdf}
}

@misc{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = aug,
  number = {arXiv:1707.06347},
  eprint = {1707.06347},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.06347},
  urldate = {2025-07-23},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a ``surrogate'' objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {,Computer Science - Machine Learning},
  file = {C:\Users\LinG\Zotero\storage\8YFDUB72\Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf}
}

@article{surDeepReinforcementLearningBased2022,
  title = {A {{Deep Reinforcement Learning-Based Scheme}} for {{Solving Multiple Knapsack Problems}}},
  author = {Sur, Giwon and Ryu, Shun Yuel and Kim, JongWon and Lim, Hyuk},
  year = {2022},
  month = mar,
  journal = {Applied Sciences},
  volume = {12},
  number = {6},
  pages = {3068},
  issn = {2076-3417},
  doi = {10.3390/app12063068},
  urldate = {2025-06-28},
  abstract = {A knapsack problem is to select a set of items that maximizes the total profit of selected items while keeping the total weight of the selected items no less than the capacity of the knapsack. As a generalized form with multiple knapsacks, the multi-knapsack problem (MKP) is to select a disjointed set of items for each knapsack. To solve MKP, we propose a deep reinforcement learning (DRL) based approach, which takes as input the available capacities of knapsacks, total profits and weights of selected items, and normalized profits and weights of unselected items and determines the next item to be mapped to the knapsack with the largest available capacity. To expedite the learning process, we adopt the Asynchronous Advantage Actor-Critic (A3C) for the policy model. The experimental results indicate that the proposed method outperforms the random and greedy methods and achieves comparable performance to an optimal policy in terms of the profit ratio of the selected items to the total profit sum, particularly when the profits and weights of items have a non-linear relationship such as quadratic forms.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {,/reading},
  annotation = {TLDR: The experimental results indicate that the proposed deep reinforcement learning (DRL) based approach to MKP outperforms the random and greedy methods and achieves comparable performance to an optimal policy in terms of the profit ratio of the selected items to the total profit sum.\\
remark: e2e-MKP, Asynchronous Advantage Actor-Critic + simplify action space},
  file = {C:\Users\LinG\Zotero\storage\YVRJXG8E\Sur et al. - 2022 - A Deep Reinforcement Learning-Based Scheme for Solving Multiple Knapsack Problems.pdf}
}

@misc{tamarValueIterationNetworks2017,
  title = {Value {{Iteration Networks}}},
  author = {Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
  year = {2017},
  month = mar,
  number = {arXiv:1602.02867},
  eprint = {1602.02867},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1602.02867},
  urldate = {2025-04-29},
  abstract = {We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C:\Users\LinG\Zotero\storage\QVNBE75I\Tamar et al. - 2017 - Value Iteration Networks.pdf}
}

@article{wangReinforcementLearningTraveling2023,
  title = {Reinforcement Learning for the Traveling Salesman Problem: {{Performance}} Comparison of Three Algorithms},
  shorttitle = {Reinforcement Learning for the Traveling Salesman Problem},
  author = {Wang, Jiaying and Xiao, Chenglong and Wang, Shanshan and Ruan, Yaqi},
  year = {2023},
  month = sep,
  journal = {The Journal of Engineering},
  volume = {2023},
  number = {9},
  pages = {e12303},
  issn = {2051-3305, 2051-3305},
  doi = {10.1049/tje2.12303},
  urldate = {2025-06-28},
  abstract = {Travelling salesman problem (TSP) is one of the most famous problems in graph theory, as well as one of the typical nondeterministic polynomial time (NP)-hard problems in combinatorial optimization. Reinforcement learning (RL) has been widely regarded as an effective tool for solving combinatorial optimization problems. This paper attempts to solve the TSP problem using different reinforcement learning algorithms and evaluated the performance of three RL algorithms (Q-Learning, SARSA, and Double Q-Learning) under different reward functions, {$E$}-greedy decay strategies, and running times. A comprehensive analysis and comparison of the three algorithms mentioned above were conducted in the experiment. First, the experimental results indicate that the Double Q-Learning algorithm is the best algorithm. Among the eight TSP instances, the Double Q-Learning algorithm outperforms the other two algorithms in five instances. Additionally, it has shorter runtimes compared to the SARSA algorithm and similar runtimes to the Q-Learning algorithm across all instances. Second, upon analysing the results, it was found that using the 1/di j reward strategy contributes to obtaining the best results for all algorithms. Among the 24 combinations of 3 algorithms and 8 instances, 17 combinations achieved the best results when the reward strategy was set to 1/di j .},
  langid = {english},
  keywords = {_unread,/unread},
  annotation = {TLDR: This paper attempts to solve the TSP problem using different reinforcement learning algorithms and evaluated the performance of three RL algorithms under different reward functions, {$\varepsilon$}-greedy decay strategies, and running times.},
  file = {C\:\\Users\\LinG\\Zotero\\storage\\D4PVZG9Y\\1707.06347v2.pdf;C\:\\Users\\LinG\\Zotero\\storage\\EMWNYXWA\\Wang et al. - 2023 - Reinforcement learning for the traveling salesman problem Performance comparison of three algorithm.pdf;C\:\\Users\\LinG\\Zotero\\storage\\JECB795D\\1-s2.0-S0360835224008350-main.pdf;C\:\\Users\\LinG\\Zotero\\storage\\KJY2AUSQ\\Solving_Combinatorial_Optimization_Problems_with_Deep_Neural_Network_A_Survey.pdf;C\:\\Users\\LinG\\Zotero\\storage\\L64WM6AQ\\2312.11739v1.pdf;C\:\\Users\\LinG\\Zotero\\storage\\XURJSP9B\\1-s2.0-S0957417422021716-main.pdf}
}

@article{wangSolvingCombinatorialOptimization2024,
  title = {Solving {{Combinatorial Optimization Problems}} with {{Deep Neural Network}}: {{A Survey}}},
  shorttitle = {Solving {{Combinatorial Optimization Problems}} with {{Deep Neural Network}}},
  author = {Wang, Feng and He, Qi and Li, Shicheng},
  year = {2024},
  month = oct,
  journal = {Tsinghua Science and Technology},
  volume = {29},
  number = {5},
  pages = {1266--1282},
  publisher = {Tsinghua University Press},
  issn = {1007-0214},
  doi = {10.26599/tst.2023.9010076},
  urldate = {2025-07-23},
  abstract = {Combinatorial Optimization Problems (COPs) are a class of optimization problems that are commonly encountered in industrial production and everyday life. Over the last few decades, traditional algorithms, such as exact algorithms, approximate algorithms, and heuristic algorithms, have been proposed to solve COPs. However, as COPs in the real world become more complex, traditional algorithms struggle to generate optimal solutions in a limited amount of time. Since Deep Neural Networks (DNNs) are not heavily dependent on expert knowledge and are adequately flexible for generalization to various COPs, several DNN-based algorithms have been proposed in the last ten years for solving COPs. Herein, we categorize these algorithms into four classes and provide a brief overview of their applications in real-world problems.},
  langid = {english},
  file = {C:\Users\LinG\Zotero\storage\HDDHFC8M\Wang et al. - 2024 - Solving Combinatorial Optimization Problems with Deep Neural Network A Survey.pdf}
}

@article{xuDeepNeuralNetwork2020,
  title = {Deep {{Neural Network Approximated Dynamic Programming}} for {{Combinatorial Optimization}}},
  author = {Xu, Shenghe and Panwar, Shivendra S. and Kodialam, Murali and Lakshman, T.V.},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {02},
  pages = {1684--1691},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i02.5531},
  urldate = {2025-04-29},
  abstract = {In this paper, we propose a general framework for combining deep neural networks (DNNs) with dynamic programming to solve combinatorial optimization problems. For problems that can be broken into smaller subproblems and solved by dynamic programming, we train a set of neural networks to replace value or policy functions at each decision step. Two variants of the neural network approximated dynamic programming (NDP) methods are proposed; in the value-based NDP method, the networks learn to estimate the value of each choice at the corresponding step, while in the policybased NDP method the DNNs only estimate the best decision at each step. The training procedure of the NDP starts from the smallest problem size and a new DNN for the next size is trained to cooperate with previous DNNs. After all the DNNs are trained, the networks are fine-tuned together to further improve overall performance. We test NDP on the linear sum assignment problem, the traveling salesman problem and the talent scheduling problem. Experimental results show that NDP can achieve considerable computation time reduction on hard problems with reasonable performance loss. In general, NDP can be applied to reducible combinatorial optimization problems for the purpose of computation time reduction.},
  copyright = {https://www.aaai.org},
  langid = {english},
  keywords = {,/done},
  annotation = {TLDR: A general framework for combining deep neural networks (DNNs) with dynamic programming to solve combinatorial optimization problems and results show that NDP can achieve considerable computation time reduction on hard problems with reasonable performance loss.\\
remark: notRL},
  file = {C:\Users\LinG\Zotero\storage\KJFQ4YFI\Xu et al. - 2020 - Deep Neural Network Approximated Dynamic Programming for Combinatorial Optimization.pdf}
}

@article{yangBoostingDynamicProgramming2018,
  title = {Boosting {{Dynamic Programming}} with {{Neural Networks}} for {{Solving NP-hard Problems}}},
  author = {Yang, Feidiao and Jin, Tiancheng and Liu, Tie-Yan and Sun, Xiaoming and Zhang, Jialin},
  year = {2018},
  month = jan,
  abstract = {Dynamic programming is a powerful method for solving combinatorial optimization problems. However, it does not always work well, particularly for some NP-hard problems having extremely large state spaces. In this paper, we propose an approach to boost the capability of dynamic programming with neural networks. First, we replace the conventional tabular method with neural networks of polynomial sizes to approximately represent dynamic programming functions. And then we design an iterative algorithm to train the neural network with data generated from a solution reconstruction process. Our method combines the approximating ability and flexibility of neural networks and the advantage of dynamic programming in utilizing intrinsic properties of a problem. This approach can significantly reduce the space complexity and it is flexible in balancing space, running time, and accuracy. We apply the method to the Travelling Salesman Problem (TSP). The experimental results show that our approach can solve larger problems that are intractable for conventional dynamic programming and the performances are near optimal, outperforming the well-known approximation algorithms.},
  langid = {english},
  keywords = {,_read,/reading},
  annotation = {remark: guide-RL(similar to policy gradient) to approxiamate DP state value space},
  file = {C:\Users\LinG\Zotero\storage\BSEXVL9U\Yang et al. - Boosting Dynamic Programming with Neural Networks for Solving NP-hard Problems.pdf}
}

@article{yildizReinforcementLearningUsing2022,
  title = {Reinforcement Learning Using Fully Connected, Attention, and Transformer Models in Knapsack Problem Solving},
  author = {Yildiz, Beytullah},
  year = {2022},
  month = apr,
  journal = {Concurrency and Computation: Practice and Experience},
  volume = {34},
  number = {9},
  pages = {e6509},
  issn = {1532-0626, 1532-0634},
  doi = {10.1002/cpe.6509},
  urldate = {2025-06-12},
  abstract = {Knapsack is a combinatorial optimization problem that involves a variety of resource allocation challenges. It is defined as non-deterministic polynomial time (NP) hard and has a wide range of applications. Knapsack problem (KP) has been studied in applied mathematics and computer science for decades. Many algorithms that can be classified as exact or approximate solutions have been proposed. Under the category of exact solutions, algorithms such as branch-and-bound and dynamic programming and the approaches obtained by combining these algorithms can be classified. Due to the fact that exact solutions require a long processing time, many approximate methods have been introduced for knapsack solution. In this research, deep Q-learning using models containing fully connected layers, attention, and transformer as function estimators were used to provide the solution for KP. We observed that deep Q-networks, which continued their training by observing the reward signals provided by the knapsack environment we developed, optimized the total reward gained over time. The results showed that our approaches give near-optimum solutions and work about 40 times faster than an exact algorithm using dynamic programming.},
  langid = {english},
  keywords = {,_read,/reading},
  annotation = {TLDR: Deep Q-learning using models containing fully connected layers, attention, and transformer as function estimators were used to provide the solution for KP and it was observed that deep Q-networks, which continued their training by observing the reward signals provided by the knapsack environment, optimized the total reward gained over time.\\
remark: e2e-DQN + transformer(better than FC, ATT.)},
  file = {C:\Users\LinG\Zotero\storage\T3AC9YPI\Yildiz - 2022 - Reinforcement learning using fully connected, attention, and transformer models in knapsack problem.pdf}
}

@article{zhangReinforcementLearningSolving2025,
  title = {Reinforcement {{Learning}} for {{Solving}} the {{Knapsack Problem}}},
  author = {Zhang, Zhenfu and Yin, Haiyan and Zuo, Liudong and Lai, Pan},
  year = {2025},
  journal = {Computers, Materials \& Continua},
  volume = {0},
  number = {0},
  pages = {1--10},
  issn = {1546-2226},
  doi = {10.32604/cmc.2025.062980},
  urldate = {2025-04-29},
  abstract = {The knapsack problem is a classical combinatorial optimization problem widely encountered in areas such as logistics, resource allocation, and portfolio optimization. Traditional methods, including dynamic programming (DP) and greedy algorithms, have been effective in solving small problem instances but often struggle with scalability and efficiency as the problem size increases. DP, for instance, has exponential time complexity and can become computationally prohibitive for large problem instances. On the other hand, greedy algorithms offer faster solutions but may not always yield the optimal results, especially when the problem involves complex constraints or large numbers of items. This paper introduces a novel reinforcement learning (RL) approach to solve the knapsack problem by enhancing the state representation within the learning environment. We propose a representation where item weights and volumes are expressed as ratios relative to the knapsack's capacity, and item values are normalized to represent their percentage of the total value across all items. This novel state modification leads to a 5\% improvement in accuracy compared to the state-of-the-art RL-based algorithms, while significantly reducing execution time. Our RL-based method outperforms DP by over 9000 times in terms of speed, making it highly scalable for larger problem instances. Furthermore, we improve the performance of the RL model by incorporating Noisy layers into the neural network architecture. The addition of Noisy layers enhances the exploration capabilities of the agent, resulting in an additional accuracy boost of 0.2\%--0.5\%. The results demonstrate that our approach not only outperforms existing RL techniques, such as the Transformer model in terms of accuracy, but also provides a substantial improvement than DP in computational efficiency. This combination of enhanced accuracy and speed presents a promising solution for tackling large-scale optimization problems in real-world applications, where both precision and time are critical factors.},
  langid = {english},
  keywords = {,/reading},
  annotation = {TLDR: A novel reinforcement learning (RL) approach to solve the knapsack problem by enhancing the state representation within the learning environment that provides a substantial improvement than DP in computational efficiency and enhances accuracy and speed.\\
remark: e2e-Dueling DQN+ normalization + noise layers},
  file = {C:\Users\LinG\Zotero\storage\55BCQIDI\Zhang et al. - 2025 - Reinforcement Learning for Solving the Knapsack Problem.pdf}
}

@article{zhaoReinforcementLearningdrivenCooperative2024,
  title = {A Reinforcement Learning-Driven Cooperative Scatter Search for the Knapsack Problem with Forfeits},
  author = {Zhao, Juntao and Hifi, Mhand},
  year = {2024},
  month = dec,
  journal = {Computers \& Industrial Engineering},
  volume = {198},
  pages = {110713},
  publisher = {Elsevier BV},
  issn = {0360-8352},
  doi = {10.1016/j.cie.2024.110713},
  urldate = {2025-07-23},
  abstract = {The knapsack problem with forfeits belongs to the NP-hard combinatorial optimization family and arises in various applications like resource allocation, finance, and logistics, where certain item combinations incur penalties. Efficiently solving such a problem is crucial for optimizing resources while minimizing penalties. This paper proposes a novel reinforcement learning-driven cooperative scatter search algorithm to solve it, combining the robust exploration capabilities of scatter search with the adaptive learning strengths of {$Q$}-learning. The algorithm starts by generating a diverse archive set to ensure broad exploration of the solution space. It then iteratively generates and combines subsets using path-relinking, followed by a two-stage improvement process: {$Q$}-learning for dynamic enhancement and a tabu-based local search for refinement. Experimental evaluations on benchmark instances highlight the proposed method's competitiveness against state-of-the-art approaches. The method establishes new lower bounds on 22 instances and matches existing bounds on others.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {C:\Users\LinG\Zotero\storage\5QRWIKIS\Zhao and Hifi - 2024 - A reinforcement learning-driven cooperative scatter search for the knapsack problem with forfeits.pdf}
}

@misc{vinyalsPointerNetworks2017,
  title = {Pointer {{Networks}}},
  author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  year = {2017},
  month = jan,
  number = {arXiv:1506.03134},
  eprint = {1506.03134},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1506.03134},
  urldate = {2025-08-28},
  abstract = {We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems -- finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem -- using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C:\Users\LinG\Zotero\storage\EICI6USQ\Vinyals et al. - 2017 - Pointer Networks.pdf}
}
